# Project 1: Data Modeling with PostgreSQL
***
## Project Goals  
The  purpose of this project is to create a Postgres database schema and ETL pipeline for a startup called Sparkify that wants to analyze the data they've been collecting on songs and user activity on their new music streaming app stored on a file system as JSON files. This optimized database will enable the company analitics team to easily write queries or use advanced Data Analytics tools (like Power BI or Tableau) to understand customer listening behaviour and patterns.


## Project Datas  
The Project datas comes from 2 datasets  
**1. Song Dataset**  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

**2. Log Dataset**  
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.  


## Database Schema Design  
Below is the schema for SongPlay Analysis. Its a star schema optimized for queries, since it has only 5 tables that can be easily joinned to do analysis: a **Fact Table** with 1 primary key (PK) that have columns of foreign keys (FK) of 4 **Dimension Tables**.  

**Fact Table**  
**1. songplays:** records in log data associated with song plays  
    songplay_id(PRIMARY KEY), start_time(FK), user_id (FK), level, song_id(FK), artist_id(FK), session, location, user_agent  
    
**Dimension Tables**  
**2. users:** users in the app  
    user_id(PRIMARY KEY), firstName, lastName, gender, level  

**3. songs:** songs in music database  
    song_id (PRIMARY KEY), title, artist_id, year, duration  
    
**4. artists:** artists in music database  
    artist_id(PRIMARY KEY), artist_name, artist_location, artist_latitude, artist_longitude  
    
**5. time:** timestamps of records in songplays broken down into specific units  
    start_time (PRIMARY KEY), hour, day , week, month, year, weekday  
    

## ETL Pipeline  
The ETL process connects to database, processes song_data and log_data datasets, extracts the users, songs, artists and time information from files and finally insert it into the created tables. Below are the steps followed to setup ETL pipeline:  

**1. Create tables**  
    On terminal Run create_tables.py to create database sparkifydb and tables: python3 create_tables.py  
    
**2. Confirm the creation of tables**  
    Test tables using test.ipynb jupyter notebook  
    
**3. Perform the ETL pipeline testing**  
    Follow instructions in etl.ipynb and test tables using test.ipynb jupyter notebook  
    
**4. Reset tables**  
    On terminal ReRun create_tables.py to reset tables: python3 create_tables.py    
    
**5.Build ETL Pipeline**  
    On terminal Run etl.py: python3 etl.py  
  
**6. Confirm records were successfully inserted into each table**  
     Run test.ipynb  

## Tables Output
Below are the screenshots of the tables created with the records inserted, generated by a SELECT query limited to 5 rows.  

**Songplays Table**  
![Songplays](/images/songplays_table.png)  


**User Table**  
![Users](/images/users_table.png)  


**Songs Table**  
![Songs](/images/songs_table.png)  


**Artists Table**  
![Artists](/images/artists_table.png)  


**Time Table**  
![Time](/images/time_table.png)  

